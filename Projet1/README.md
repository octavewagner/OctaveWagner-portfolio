# Projet 1 : Mod√®le pr√©dictif en NBA sur la saison 21-22

## Introduction
Dans le cadre de mon master √† l'Universit√© Panth√©on - Sorbonne, j'ai r√©alis√© un projet complet de Machine Learning visant √† pr√©dire le pourcentage de victoire d'une √©quipe NBA versus une autre, toute conf√©rence confondue. Voir le PDF pour plus de d√©tails

Les donn√©es sont issus d'un site sp√©cialis√© en data analyse : Predictive Hacks.

Ce projet a √©t√© con√ßu pour d√©montrer mes comp√©tences en collecte de donn√©es (web scrapping), structuration des donn√©es et mod√©lisation pr√©dictive.

## Objectif üéØ
L'objectif principal de ce projet √©tait de construire l'un des meilleurs mod√®les pr√©dictifs possibles. 
J'ai compar√© 6 algorithmes diff√©rents sur la base d'un indicateur de fiabilit√© : 

- R√©gression logistique
- Arbre de r√©gression
- F√¥ret al√©atoire
- Naive Baysien
- KNN
- R√©seau de neurones

La construction du mod√®le s'est faite sur les diff√©rentes statistiques sur une saison r√©guli√®re comme le taux de victoire, le taux de victoire √† domicile et bien d'autres. 

## √âtapes du Projet ‚õ∞Ô∏è

### Collecte de Donn√©es 
- Collecte des donn√©es en utilisant le tuto suivant : https://predictivehacks.com/how-to-build-a-predictive-model-for-nba-games/
1. Web scrapping sur le site de la ESPN des scores NBA avec leurs statistiques (points, rebonds, assitances)
2. Transformation des donn√©es en indicateurs pertinents (taux de victoire...)
3. Selection des variables n√©cessaires et utiles pour un mod√®le de Machine Learning

Base de donn√©es initiale : 
![BdD NBA](https://github.com/user-attachments/assets/0c61a28e-bc65-4d91-9003-df157d5c9d9f)

Base de donn√©es transform√© : 
![BdD NBA final](https://github.com/user-attachments/assets/ff0ac821-9434-47db-818c-d8ee21c2b4dc)

### Stockage et Nettoyage des Donn√©es
- Stockage et structuration des donn√©es dans l'IDE Rstudio. Chaque algorithme a des besoins sp√©ciques en entr√©e.

### Algortihme 1 : Logit

- Entr√©es : Utilisation de caract√©ristiques num√©riques. Transformation automatiques des variables "character" avec la fonction glm()
- Sorties : Pr√©diction des r√©sultats entre une √©quipe A et une √©quipe B
- Simplification du mod√®le avec la fonction step()
- Indicateur de fiabilit√© : AUC de 0,7327 > 0,5. Plutot bon

### Algortihme 2 : Arbre de r√©gression
- Entr√©e : Utilisation de caract√©ristiques num√©riques + character
- Sorties : Arbre de d√©cision selon des crit√®res
- √âlagage de l'arbre avec CP = 0,02 qui minimise le taux d'erreur
- Indicateur de fiabilit√© : AUC de 0,5953 > 0,50. Pas terrible

### Algortihme 3 : Foret al√©atoire
- Entr√©e : Utilisation de caract√©ristiques num√©riques + character 
- Sorties : G√©n√©ralisation de plusieurs arbres de regression
- Choix du nombres d'arbre : tress = 80 qui minimise le taux d'erreur
- Indicateur de fiabilit√© : AUC de 0,6209. Mieux mais toujours pas terrible


### Algortihme 4 : Naive Baysien 
- Entr√©e : Utilisation de caract√©ristiques num√©riques + character
- Sorties : Pr√©diction des r√©sultats entre une √©quipe A et une √©quipe B
- Correcteur de Laplace = 1
- Indicateur de fiabilit√© : AUC de 0,717 > 0,50. Plutot bon


### Algortihme 5 : KNN
- Entr√©e : Selection des caract√©ristiques uniquement num√©riques + Target en num√©rque (H_Outcome)
- Sortie : Pr√©diction des r√©sultats entre une √©quipe A et une √©quipe B
- Centrer et r√©duire les donn√©es pour remettre √† l'√©chelle
- Choix du nombre de voisins : k = 29 qui repr√©sente le plus faible taux d'erreur
- 
### Algortihme 6 : R√©seau de neuronnes

